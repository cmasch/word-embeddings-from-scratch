{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: \t3.2.0\n",
      "TensorFlow version: \t1.1.0\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging, os, re, string, tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "print('gensim version: \\t%s'     % gensim.__version__)\n",
    "print('TensorFlow version: \\t%s' % tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For displaying gensim logs\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Directory with raw txt-files\n",
    "TEXT_DIR  = 'data/yelp/train'\n",
    "\n",
    "# Directory for saving checkpoint and metadata\n",
    "MODEL_DIR = 'emb_yelp/'\n",
    "\n",
    "# Word2vec\n",
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 200000\n"
     ]
    }
   ],
   "source": [
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Cleaning a document by several methods\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    doc = doc.lower()\n",
    "    # Remove numbers\n",
    "    doc = re.sub(r\"[0-9]+\", \"\", doc)\n",
    "    # Split in tokens\n",
    "    tokens = doc.split()\n",
    "    # Remove punctuation\n",
    "    tokens = [w.translate(str.maketrans('', '', string.punctuation)) for w in tokens]\n",
    "    # Tokens with less then two characters will be ignored\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def read_files(path):\n",
    "    \"\"\"\n",
    "    Read in text files\n",
    "    \"\"\"\n",
    "    documents = list()\n",
    "    tokenize  = lambda x: simple_preprocess(x)\n",
    "    \n",
    "    # Read in all files in directory\n",
    "    if os.path.isdir(path):\n",
    "        for filename in os.listdir(path):\n",
    "            with open('%s/%s' % (path, filename), encoding='utf-8') as f:\n",
    "                doc = f.read()\n",
    "                doc = clean_doc(doc)\n",
    "                documents.append(tokenize(doc))\n",
    "    return documents\n",
    "\n",
    "docs = read_files(TEXT_DIR)\n",
    "print('Number of documents: %i' % len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 1332433 words, keeping 34615 word types\n",
      "INFO : PROGRESS: at sentence #20000, processed 2682863 words, keeping 51033 word types\n",
      "INFO : PROGRESS: at sentence #30000, processed 4013459 words, keeping 63788 word types\n",
      "INFO : PROGRESS: at sentence #40000, processed 5386954 words, keeping 75163 word types\n",
      "INFO : PROGRESS: at sentence #50000, processed 6725433 words, keeping 85157 word types\n",
      "INFO : PROGRESS: at sentence #60000, processed 8069244 words, keeping 94511 word types\n",
      "INFO : PROGRESS: at sentence #70000, processed 9429856 words, keeping 103236 word types\n",
      "INFO : PROGRESS: at sentence #80000, processed 10766708 words, keeping 111540 word types\n",
      "INFO : PROGRESS: at sentence #90000, processed 12117230 words, keeping 119401 word types\n",
      "INFO : PROGRESS: at sentence #100000, processed 13487814 words, keeping 127178 word types\n",
      "INFO : PROGRESS: at sentence #110000, processed 14465090 words, keeping 134272 word types\n",
      "INFO : PROGRESS: at sentence #120000, processed 15432222 words, keeping 141021 word types\n",
      "INFO : PROGRESS: at sentence #130000, processed 16408624 words, keeping 147231 word types\n",
      "INFO : PROGRESS: at sentence #140000, processed 17386678 words, keeping 153494 word types\n",
      "INFO : PROGRESS: at sentence #150000, processed 18368502 words, keeping 159326 word types\n",
      "INFO : PROGRESS: at sentence #160000, processed 19358391 words, keeping 165233 word types\n",
      "INFO : PROGRESS: at sentence #170000, processed 20342281 words, keeping 170920 word types\n",
      "INFO : PROGRESS: at sentence #180000, processed 21323355 words, keeping 176227 word types\n",
      "INFO : PROGRESS: at sentence #190000, processed 22297528 words, keeping 181834 word types\n",
      "INFO : collected 186895 word types from a corpus of 23280688 raw words and 200000 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=5 retains 42113 unique words (22% of original 186895, drops 144782)\n",
      "INFO : min_count=5 leaves 23078591 word corpus (99% of original 23280688, drops 202097)\n",
      "INFO : deleting the raw counts dictionary of 186895 items\n",
      "INFO : sample=0.001 downsamples 58 most-common words\n",
      "INFO : downsampling leaves estimated 17763378 word corpus (77.0% of prior 23078591)\n",
      "INFO : estimated required memory for 42113 words and 300 dimensions: 122127700 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 3 workers on 42113 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "INFO : PROGRESS: at 0.52% examples, 527873 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 1.07% examples, 539312 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : PROGRESS: at 1.60% examples, 541588 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 2.15% examples, 545192 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 2.69% examples, 545475 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 3.22% examples, 543218 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 3.76% examples, 545142 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 4.31% examples, 546955 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 4.84% examples, 546151 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 5.39% examples, 546073 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 5.92% examples, 545658 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : PROGRESS: at 6.46% examples, 545864 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 6.98% examples, 545495 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 7.52% examples, 545294 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 8.07% examples, 545783 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 8.62% examples, 546252 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 9.16% examples, 546600 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 9.69% examples, 546823 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 10.30% examples, 546768 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 11.04% examples, 546609 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 11.80% examples, 546783 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 12.52% examples, 546174 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 13.24% examples, 546075 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 14.00% examples, 546585 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 14.73% examples, 546676 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 15.47% examples, 546709 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 16.18% examples, 546396 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 16.94% examples, 546453 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 17.67% examples, 546600 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 18.39% examples, 546197 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : PROGRESS: at 19.14% examples, 546115 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 19.88% examples, 546115 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 20.46% examples, 546388 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : PROGRESS: at 21.01% examples, 546688 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 21.55% examples, 546876 words/s, in_qsize 6, out_qsize 0\n",
      "INFO : PROGRESS: at 22.11% examples, 547233 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 22.66% examples, 547603 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 23.20% examples, 547571 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 23.74% examples, 547800 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 24.28% examples, 547952 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 24.83% examples, 548022 words/s, in_qsize 3, out_qsize 2\n",
      "INFO : PROGRESS: at 25.36% examples, 547882 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 25.89% examples, 548033 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 26.44% examples, 547905 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 26.98% examples, 548200 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 27.52% examples, 548113 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 28.07% examples, 548427 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 28.61% examples, 548230 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 29.14% examples, 547988 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 29.66% examples, 548090 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 30.27% examples, 548131 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 31.04% examples, 548392 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 31.79% examples, 548429 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 32.52% examples, 548368 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 33.24% examples, 548270 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 33.95% examples, 547915 words/s, in_qsize 3, out_qsize 2\n",
      "INFO : PROGRESS: at 34.70% examples, 547991 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 35.43% examples, 547805 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 36.13% examples, 547498 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 36.88% examples, 547647 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 37.63% examples, 547632 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 38.36% examples, 547557 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 39.12% examples, 547681 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 39.86% examples, 547717 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 40.45% examples, 547849 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 41.01% examples, 548044 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 41.55% examples, 548228 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 42.10% examples, 548395 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 42.66% examples, 548437 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 43.20% examples, 548535 words/s, in_qsize 3, out_qsize 2\n",
      "INFO : PROGRESS: at 43.75% examples, 548726 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 44.29% examples, 548860 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 44.84% examples, 548925 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 45.39% examples, 549001 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: at 45.92% examples, 549012 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 46.49% examples, 549227 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 47.02% examples, 549361 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 47.57% examples, 549318 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 48.12% examples, 549365 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 48.66% examples, 549447 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 49.21% examples, 549622 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 49.75% examples, 549740 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 50.39% examples, 549725 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 51.15% examples, 549719 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 51.92% examples, 549813 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 52.65% examples, 549782 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 53.41% examples, 549901 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 54.15% examples, 549811 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 54.90% examples, 549840 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 55.62% examples, 549644 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 56.36% examples, 549618 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 57.09% examples, 549449 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 57.81% examples, 549398 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 58.55% examples, 549401 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 59.28% examples, 549371 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 60.03% examples, 549347 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 60.57% examples, 549318 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 61.12% examples, 549323 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : PROGRESS: at 61.64% examples, 549296 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 62.19% examples, 549347 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 62.75% examples, 549331 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 63.29% examples, 549420 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 63.81% examples, 549315 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 64.35% examples, 549314 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 64.89% examples, 549266 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 65.44% examples, 549330 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 65.97% examples, 549304 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 66.51% examples, 549280 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 67.03% examples, 549260 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 67.60% examples, 549395 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 68.15% examples, 549503 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 68.69% examples, 549521 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 69.23% examples, 549548 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 69.76% examples, 549564 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 70.39% examples, 549458 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 71.12% examples, 549349 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 71.88% examples, 549374 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 72.61% examples, 549396 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 73.35% examples, 549361 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 74.10% examples, 549307 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 74.83% examples, 549175 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 75.57% examples, 549201 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 76.31% examples, 549271 words/s, in_qsize 4, out_qsize 1\n",
      "INFO : PROGRESS: at 77.05% examples, 549209 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 77.79% examples, 549232 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 78.56% examples, 549267 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 79.29% examples, 549316 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 80.03% examples, 549249 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 80.59% examples, 549350 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 81.15% examples, 549341 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 81.69% examples, 549397 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 82.24% examples, 549470 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 82.79% examples, 549511 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 83.33% examples, 549472 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 83.87% examples, 549573 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 84.41% examples, 549599 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 84.96% examples, 549604 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 85.50% examples, 549616 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 86.05% examples, 549657 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 86.58% examples, 549599 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 87.10% examples, 549618 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 87.67% examples, 549664 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 88.21% examples, 549730 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 88.74% examples, 549650 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 89.29% examples, 549723 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 89.83% examples, 549779 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 90.52% examples, 549807 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 91.27% examples, 549750 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 92.01% examples, 549717 words/s, in_qsize 6, out_qsize 1\n",
      "INFO : PROGRESS: at 92.76% examples, 549762 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 93.49% examples, 549764 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 94.24% examples, 549758 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 94.99% examples, 549757 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 95.72% examples, 549731 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 96.46% examples, 549755 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 97.20% examples, 549792 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 97.94% examples, 549756 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 98.68% examples, 549703 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : PROGRESS: at 99.42% examples, 549712 words/s, in_qsize 5, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : training on 116403440 raw words (88814925 effective words) took 161.6s, 549745 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(docs, size=EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving Word2Vec object under emb_yelp/word2vec, separately None\n",
      "INFO : storing np array 'syn0' to emb_yelp/word2vec.wv.syn0.npy\n",
      "INFO : not storing attribute syn0norm\n",
      "INFO : storing np array 'syn1neg' to emb_yelp/word2vec.syn1neg.npy\n",
      "INFO : not storing attribute cum_table\n",
      "INFO : saved emb_yelp/word2vec\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
	"    os.makedirs(MODEL_DIR)\n",
    "model.save(os.path.join(MODEL_DIR,'word2vec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating checkpoint and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights: (42113, 300)\n",
      "Vocabulary size: 42113\n",
      "Embedding size: 300\n"
     ]
    }
   ],
   "source": [
    "weights     = model.wv.vectors\n",
    "index_words = model.wv.index2word\n",
    "\n",
    "vocab_size    = weights.shape[0]\n",
    "embedding_dim = weights.shape[1]\n",
    "\n",
    "print('Shape of weights:', weights.shape)\n",
    "print('Vocabulary size: %i' % vocab_size)\n",
    "print('Embedding size: %i'  % embedding_dim)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR,'metadata.tsv'), 'w') as f:\n",
    "    f.writelines(\"\\n\".join(index_words))\n",
    "\n",
    "# Required if you re-run without restarting the kernel\n",
    "tf.reset_default_graph()\n",
    "    \n",
    "W = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]), trainable=False, name=\"W\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "\n",
    "embedding_init = W.assign(embedding_placeholder)\n",
    "writer = tf.summary.FileWriter(MODEL_DIR, graph=tf.get_default_graph())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = W.name\n",
    "embedding.metadata_path = os.path.join(MODEL_DIR,'metadata.tsv')\n",
    "projector.visualize_embeddings(writer, config)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(embedding_init, feed_dict={embedding_placeholder: weights})\n",
    "    save_path = saver.save(sess, os.path.join(MODEL_DIR, \"model.cpkt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('espresso', 0.6709840893745422),\n",
       " ('latte', 0.6611574292182922),\n",
       " ('cappuccino', 0.6460868716239929),\n",
       " ('tea', 0.643097996711731),\n",
       " ('lattes', 0.613446056842804),\n",
       " ('coffees', 0.612466037273407),\n",
       " ('teas', 0.5807890295982361),\n",
       " ('chai', 0.567467451095581),\n",
       " ('mocha', 0.565311074256897),\n",
       " ('gelato', 0.5606527328491211)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['coffee'], topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
